{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde35960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd9c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path (please update this part according to the location)\n",
    "path = '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64db9f",
   "metadata": {},
   "source": [
    "# Step 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b7d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fmri_ADHD_Combined = os.chdir(\"{}/ADHD_Combined/\".format(path))\n",
    "fmri_ADHD_Combined = os.listdir(path_fmri_ADHD_Combined)\n",
    "\n",
    "fmri_ADHD_Combined_values = []\n",
    "num_features = 190\n",
    "\n",
    "\n",
    "for file in fmri_ADHD_Combined:\n",
    "    each_file = []\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data = csv.reader(csvfile, delimiter = ' ')\n",
    "            data_array = []\n",
    "            for d in data:\n",
    "                float_d = []\n",
    "                for num in d:\n",
    "                    #print(num)\n",
    "                    float_d.append((float(num)))\n",
    "                data_array.append(float_d)\n",
    "            data_array = np.array(data_array).reshape(num_features, num_features)\n",
    "            for i in range(num_features):\n",
    "                for j in range(i + 1, num_features):\n",
    "                    each_file.append(data_array[i][j])\n",
    "        fmri_ADHD_Combined_values.append(each_file)\n",
    "\n",
    "header = []\n",
    "for i in range(num_features):\n",
    "    for j in range(i + 1, num_features):\n",
    "        header.append((i + 1, j + 1))\n",
    "        \n",
    "#print(header)\n",
    "fmri_ADHD_Combined_values_fr = pd.DataFrame(fmri_ADHD_Combined_values, columns = header)\n",
    "fmri_ADHD_Combined_values_fr.to_csv(\"{}/fmri_ADHD_Combined_values.csv\".format(path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69423ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fmri_ADHD_Hyperactive = os.chdir(\"{}/ADHD_Hyperactive/\".format(path))\n",
    "fmri_ADHD_Hyperactive = os.listdir(path_fmri_ADHD_Hyperactive)\n",
    "\n",
    "fmri_ADHD_Hyperactive_values = []\n",
    "num_features = 190\n",
    "\n",
    "\n",
    "for file in fmri_ADHD_Hyperactive:\n",
    "    each_file = []\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data = csv.reader(csvfile, delimiter = ' ')\n",
    "            data_array = []\n",
    "            for d in data:\n",
    "                float_d = []\n",
    "                for num in d:\n",
    "                    #print(num)\n",
    "                    float_d.append((float(num)))\n",
    "                data_array.append(float_d)\n",
    "            data_array = np.array(data_array).reshape(num_features, num_features)\n",
    "            for i in range(num_features):\n",
    "                for j in range(i + 1, num_features):\n",
    "                    each_file.append(data_array[i][j])\n",
    "        fmri_ADHD_Hyperactive_values.append(each_file)\n",
    "\n",
    "header = []\n",
    "for i in range(num_features):\n",
    "    for j in range(i + 1, num_features):\n",
    "        header.append((i + 1, j + 1))\n",
    "        \n",
    "#print(header)\n",
    "fmri_ADHD_Hyperactive_values_fr = pd.DataFrame(fmri_ADHD_Hyperactive_values, columns = header)\n",
    "fmri_ADHD_Hyperactive_values_fr.to_csv(\"{}/fmri_ADHD_Hyperactive_values.csv\".format(path), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39080011",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fmri_ADHD_Inattentive = os.chdir(\"{}/ADHD_Inattentive/\".format(path))\n",
    "fmri_ADHD_Inattentive = os.listdir(path_fmri_ADHD_Inattentive)\n",
    "\n",
    "fmri_ADHD_Inattentive_values = []\n",
    "num_features = 190\n",
    "\n",
    "\n",
    "for file in fmri_ADHD_Inattentive:\n",
    "    each_file = []\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data = csv.reader(csvfile, delimiter = ' ')\n",
    "            data_array = []\n",
    "            for d in data:\n",
    "                float_d = []\n",
    "                for num in d:\n",
    "                    #print(num)\n",
    "                    float_d.append((float(num)))\n",
    "                data_array.append(float_d)\n",
    "            data_array = np.array(data_array).reshape(num_features, num_features)\n",
    "            for i in range(num_features):\n",
    "                for j in range(i + 1, num_features):\n",
    "                    each_file.append(data_array[i][j])\n",
    "        fmri_ADHD_Inattentive_values.append(each_file)\n",
    "\n",
    "header = []\n",
    "for i in range(num_features):\n",
    "    for j in range(i + 1, num_features):\n",
    "        header.append((i + 1, j + 1))\n",
    "        \n",
    "#print(header)\n",
    "fmri_ADHD_Inattentive_values_fr = pd.DataFrame(fmri_ADHD_Inattentive_values, columns = header)\n",
    "fmri_ADHD_Inattentive_values_fr.to_csv(\"{}/fmri_ADHD_Inattentive_values.csv\".format(path), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0c9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fmri_Typical_Development = os.chdir(\"{}/Typical_Development/\".format(path))\n",
    "fmri_Typical_Development = os.listdir(path_fmri_Typical_Development)\n",
    "\n",
    "fmri_Typical_Development_values = []\n",
    "num_features = 190\n",
    "\n",
    "\n",
    "for file in fmri_Typical_Development:\n",
    "    each_file = []\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data = csv.reader(csvfile, delimiter = ' ')\n",
    "            data_array = []\n",
    "            for d in data:\n",
    "                float_d = []\n",
    "                for num in d:\n",
    "                    #print(num)\n",
    "                    float_d.append((float(num)))\n",
    "                data_array.append(float_d)\n",
    "            data_array = np.array(data_array).reshape(num_features, num_features)\n",
    "            for i in range(num_features):\n",
    "                for j in range(i + 1, num_features):\n",
    "                    each_file.append(data_array[i][j])\n",
    "        fmri_Typical_Development_values.append(each_file)\n",
    "\n",
    "header = []\n",
    "for i in range(num_features):\n",
    "    for j in range(i + 1, num_features):\n",
    "        header.append((i + 1, j + 1))\n",
    "        \n",
    "#print(header)\n",
    "fmri_Typical_Development_values_fr = pd.DataFrame(fmri_Typical_Development_values, columns = header)\n",
    "fmri_Typical_Development_values_fr.to_csv(\"{}/fmri_Typical_Development_values.csv\".format(path), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c652e",
   "metadata": {},
   "source": [
    "# Step 2. Two-sample t-test for signficant edges identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9e3f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Regions Connectivity\n",
      "0                   (1, 2)\n",
      "1                   (1, 3)\n",
      "2                   (1, 4)\n",
      "3                   (1, 5)\n",
      "4                   (1, 6)\n",
      "...                    ...\n",
      "17950           (187, 189)\n",
      "17951           (187, 190)\n",
      "17952           (188, 189)\n",
      "17953           (188, 190)\n",
      "17954           (189, 190)\n",
      "\n",
      "[17955 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "ADHD_Combined_data = pd.read_csv(\"{}/fmri_ADHD_Combined_values.csv\".format(path))\n",
    "ADHD_Hyperactive_data = pd.read_csv(\"{}/fmri_ADHD_Hyperactive_values.csv\".format(path))\n",
    "ADHD_Inattentive_data = pd.read_csv(\"{}/fmri_ADHD_Inattentive_values.csv\".format(path))\n",
    "\n",
    "ADHD_data = pd.concat([ADHD_Combined_data,ADHD_Hyperactive_data,ADHD_Inattentive_data],axis = 0)\n",
    "\n",
    "TD_data = pd.read_csv(\"{}/fmri_Typical_Development_values.csv\".format(path))\n",
    "\n",
    "columns = (ADHD_data.columns)\n",
    "ttest_stat = []\n",
    "ttest_pval = []\n",
    "\n",
    "for i in columns:\n",
    "    ASD_region_i = np.array(ADHD_data[i]).reshape(-1,1)\n",
    "    TD_region_i = np.array(TD_data[i]).reshape(-1,1)\n",
    "    t_stat, t_pval = scipy.stats.ttest_ind(ASD_region_i, TD_region_i, equal_var = False)\n",
    "    ttest_stat.append(float(t_stat))\n",
    "    ttest_pval.append(float(t_pval))\n",
    "    \n",
    "regions_fr = pd.DataFrame(columns, columns=['Regions Connectivity'])\n",
    "ttest_stat_fr = pd.DataFrame(ttest_stat, columns=['T statistics'])\n",
    "ttest_pval_fr = pd.DataFrame(ttest_pval, columns=['P-values'])\n",
    "print(regions_fr)\n",
    "\n",
    "region_1 = []\n",
    "region_2 = []\n",
    "\n",
    "\n",
    "for i in range(len(regions_fr)):\n",
    "    regions = (regions_fr.iloc[i])\n",
    "    a, b = regions[0][1:-1].split(\",\")\n",
    "    a, b = int(a), int(b)\n",
    "    region_1.append(a)\n",
    "    region_2.append(b)\n",
    "    \n",
    "regions_1 = pd.DataFrame(region_1, columns=['Region 1'])\n",
    "regions_2 = pd.DataFrame(region_2, columns=['Region 2'])\n",
    "\n",
    "t_test_results = pd.concat([regions_1,regions_2, ttest_stat_fr, ttest_pval_fr], axis = 1)\n",
    "#print(t_test_results)\n",
    "\n",
    "sign_results = t_test_results[t_test_results['P-values'] <= 0.005]\n",
    "#print(sign_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d5e36",
   "metadata": {},
   "source": [
    "# Step 3. Construct subgraphs by using edges from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac4150c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60743633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in Cluster 1: 67\n",
      "Number of edges in Cluster 2: 66\n",
      "Number of edges in Cluster 3: 66\n",
      "Number of edges in Cluster 4: 66\n",
      "Number of edges in Cluster 5: 67\n"
     ]
    }
   ],
   "source": [
    "# Determine the clusters\n",
    "\n",
    "sign_edges_list = sign_results\n",
    "quantiles = list((abs(sign_edges_list['T statistics'])).quantile([.20, .40, .60, .80]))\n",
    "\n",
    "z_1 = sign_edges_list[(abs(sign_edges_list['T statistics']) < quantiles[0])]\n",
    "z_2 = sign_edges_list[(abs(sign_edges_list['T statistics'])>= quantiles[0]) & (abs(sign_edges_list['T statistics']) < quantiles[1])]\n",
    "z_3 = sign_edges_list[(abs(sign_edges_list['T statistics'])>= quantiles[1])& (abs(sign_edges_list['T statistics']) < quantiles[2])]\n",
    "z_4 = sign_edges_list[(abs(sign_edges_list['T statistics'])>= quantiles[2])& (abs(sign_edges_list['T statistics']) < quantiles[3])]\n",
    "z_5 = sign_edges_list[(abs(sign_edges_list['T statistics'])>= quantiles[3])]\n",
    "\n",
    "print(\"Number of edges in Cluster 1:\",len(z_1))\n",
    "print(\"Number of edges in Cluster 2:\",len(z_2))\n",
    "print(\"Number of edges in Cluster 3:\",len(z_3))\n",
    "print(\"Number of edges in Cluster 4:\",len(z_4))\n",
    "print(\"Number of edges in Cluster 5:\",len(z_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8475cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group significant edges into clusters\n",
    "\n",
    "cluster_edges_list = []\n",
    "for i in range(1,6):\n",
    "    cluster_name = f'z_{i}'\n",
    "    cluster = eval(cluster_name)\n",
    "    cluster_edge = []\n",
    "    if len(cluster) != 0:\n",
    "        for j in range(len(cluster)):\n",
    "            region_1, region_2 = cluster['Region 1'].iloc[j], cluster['Region 2'].iloc[j]\n",
    "            cluster_edge.append([region_1, region_2])\n",
    "        cluster_edges_list.append(cluster_edge)\n",
    "\n",
    "\n",
    "\n",
    "qualified_sign_edges = []\n",
    "\n",
    "for i in range(len(sign_edges_list)):\n",
    "    region_1, region_2 = sign_edges_list['Region 1'].iloc[i], sign_edges_list['Region 2'].iloc[i]\n",
    "    qualified_sign_edges.append((region_1, region_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604b1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions below are to generate the subgraphs using signficant edges in each cluster, and remove those duplicates\n",
    "\n",
    "def generate_all_cycles(nodes, edges):\n",
    "    def dfs(node, start, path, visited):\n",
    "        if node == start and path and len(path) > 2:\n",
    "            cycle = tuple(sorted(set(path)))\n",
    "            if cycle not in all_cycles:  # Check for duplicates\n",
    "                all_cycles.add(cycle)\n",
    "            return\n",
    "        if visited[node]:\n",
    "            return\n",
    "        visited[node] = True\n",
    "        path.append(node)\n",
    "        for neighbor in graph[node]:\n",
    "            if not visited[neighbor] or (neighbor == start and len(path) > 1):\n",
    "                dfs(neighbor, start, path, visited.copy())\n",
    "        path.pop()\n",
    "\n",
    "    # Convert edge list to adjacency list\n",
    "    graph = {node: set() for node in nodes}\n",
    "    for u, v in edges:\n",
    "        graph[u].add(v)\n",
    "        graph[v].add(u)\n",
    "\n",
    "    all_cycles = set()\n",
    "    for node in nodes:\n",
    "        dfs(node, node, [], {n: False for n in nodes})\n",
    "\n",
    "    return [list(cycle) for cycle in all_cycles]\n",
    "\n",
    "def remove_subcycles(cycles):\n",
    "    unique_cycles = []\n",
    "    for cycle in sorted(cycles, key=len, reverse=True):\n",
    "        if not any(set(cycle).issubset(set(other_cycle)) and len(cycle) < len(other_cycle) for other_cycle in unique_cycles):\n",
    "            unique_cycles.append(cycle)\n",
    "    return unique_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a23cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1\n",
      "Number of subgraphs: 0\n",
      "\n",
      "\n",
      "Cluster 2\n",
      "Number of subgraphs: 1\n",
      "\n",
      "\n",
      "Cluster 3\n",
      "Number of subgraphs: 2\n",
      "\n",
      "\n",
      "Cluster 4\n",
      "Number of subgraphs: 0\n",
      "\n",
      "\n",
      "Cluster 5\n",
      "Number of subgraphs: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AAL has 116 ROIs\n",
    "nodes = []\n",
    "\n",
    "for i in range(1,191):\n",
    "    nodes.append(i)\n",
    "    \n",
    "\n",
    "# Here we need to check subgraphs constructed in each cluster\n",
    "cluster_index = []\n",
    "\n",
    "ordered_unique_cycles = []\n",
    "ordered_unique_cycle_edges = []\n",
    "\n",
    "for i in range(len(cluster_edges_list)):\n",
    "    edges = cluster_edges_list[i]    \n",
    "    #Generate all subgraphs\n",
    "    all_cycles = generate_all_cycles(nodes, edges)\n",
    "    #Remove duplicates\n",
    "    unique_cycle = remove_subcycles(all_cycles)\n",
    "    print(\"Cluster {}\".format(i + 1))\n",
    "    print(\"Number of subgraphs:\", len(unique_cycle))\n",
    "    if len(unique_cycle) != 0:\n",
    "        for j in range(len(unique_cycle)):\n",
    "            cluster_index.append(i + 1)\n",
    "        ordered_unique_cycle_edges.append(edges)\n",
    "        ordered_unique_cycles.append(unique_cycle)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1948e9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1\n",
      "[3, 188, 160, 172, 3]\n",
      "\t\n",
      "Graph 2\n",
      "[53, 108, 189, 126, 53]\n",
      "\t\n",
      "Graph 3\n",
      "[83, 144, 173, 150, 83]\n",
      "\t\n",
      "Graph 4\n",
      "[11, 188, 85, 13, 175, 178, 150, 100, 124, 11]\n",
      "\t\n",
      "Graph 5\n",
      "[11, 188, 85, 83, 175, 178, 150, 100, 124, 11]\n",
      "\t\n",
      "Graph 6\n",
      "[7, 56, 173, 73, 162, 52, 77, 97, 178, 7]\n",
      "\t\n",
      "Graph 7\n",
      "[11, 83, 85, 13, 175, 178, 150, 100, 124, 11]\n",
      "\t\n",
      "Graph 8\n",
      "[7, 56, 173, 73, 162, 52, 174, 178, 7]\n",
      "\t\n",
      "Graph 9\n",
      "[7, 56, 173, 73, 77, 52, 174, 178, 7]\n",
      "\t\n",
      "Graph 10\n",
      "[52, 162, 73, 77, 97, 178, 174, 52]\n",
      "\t\n",
      "Graph 11\n",
      "[11, 83, 175, 13, 85, 188, 11]\n",
      "\t\n",
      "Graph 12\n",
      "[52, 133, 181, 52]\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# This function shows how exactly the subgraphs look like (e.g., which node is connected to which one)\n",
    "\n",
    "def reorder_ring_according_to_edges(nodes, edges):\n",
    "    def dfs(node, path):\n",
    "        if len(path) == len(nodes):\n",
    "            if path[0] in graph[path[-1]]:  # Check if we can complete the cycle\n",
    "                return path + [path[0]]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        for neighbor in graph[node]:\n",
    "            if neighbor not in path:\n",
    "                new_path = dfs(neighbor, path + [neighbor])\n",
    "                if new_path:\n",
    "                    return new_path\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Convert edge list to adjacency list\n",
    "    graph = {node: set() for node in nodes}\n",
    "    for u, v in edges:\n",
    "        if u in nodes and v in nodes:  # Only consider edges within the specified nodes\n",
    "            graph[u].add(v)\n",
    "            graph[v].add(u)\n",
    "\n",
    "    # Attempt to reorder from each node\n",
    "    for start_node in nodes:\n",
    "        path = dfs(start_node, [start_node])\n",
    "        if path:\n",
    "            return path\n",
    "\n",
    "    return \"No valid reordering found.\"\n",
    "\n",
    "\n",
    "reordered_rings = []\n",
    "for i in range(len(ordered_unique_cycles)):\n",
    "    cycle = ordered_unique_cycles[i]\n",
    "    edges = ordered_unique_cycle_edges[i]\n",
    "    for node in cycle:\n",
    "        reordered_ring = reorder_ring_according_to_edges(node, edges)\n",
    "        reordered_rings.append(reordered_ring)\n",
    "        \n",
    "reordered_rings_info = []\n",
    "\n",
    "for i in range(len(reordered_rings)):\n",
    "    print(\"Graph {}\".format(i + 1))\n",
    "    print(reordered_rings[i])\n",
    "    reordered_rings_info.append(reordered_rings[i][:-1])\n",
    "    print(\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aacb7b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_cycles = []\n",
    "\n",
    "\n",
    "for cycle_list in (ordered_unique_cycles):\n",
    "    for cycle in cycle_list:\n",
    "        filtered_cycles.append(list(cycle))\n",
    "\n",
    "filtered_cycles_fr = pd.DataFrame((filtered_cycles)).astype(\"Int32\")\n",
    "cluster_index_fr = pd.DataFrame(cluster_index, columns = ['Cluster'])\n",
    "filtered_cycles_with_clusters = pd.concat([cluster_index_fr, filtered_cycles_fr], axis = 1)\n",
    "filtered_cycles_fr.to_csv(\"{}/ring_nodes_5_clusters.csv\".format(path),index = False)\n",
    "\n",
    "original_list = cluster_edges_list\n",
    "\n",
    "# Converting each inner list to a tuple\n",
    "modified_list = [[tuple(inner_list) for inner_list in outer_list] for outer_list in original_list]\n",
    "\n",
    "loop_edge = []\n",
    "\n",
    "for idx in range(len(filtered_cycles_with_clusters)):\n",
    "    cycle_row = filtered_cycles_with_clusters.iloc[idx]\n",
    "    cluster = cycle_row['Cluster'] - 1\n",
    "    cycle = list((cycle_row.iloc[1:]).dropna())\n",
    "    combines_two = list(combinations(cycle, 2))\n",
    "    each_cycle_sign_edge = []\n",
    "    for comb in combines_two:\n",
    "        if comb in modified_list[cluster]:\n",
    "            each_cycle_sign_edge.append(comb)            \n",
    "    loop_edge.append(each_cycle_sign_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e451454",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_edge_fr = pd.DataFrame(loop_edge)\n",
    "loop_edge_fr.to_csv(\"{}/ring_topology_5_clusters.csv\".format(path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8811099",
   "metadata": {},
   "source": [
    "# Step 4. Extract Eigen-Entropies from all subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "491a0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def eigen_entropy(correlation_array):\n",
    "    w,v = np.linalg.eig((correlation_array))\n",
    "    w = np.real_if_close(w,tol=1)\n",
    "    w_abs = abs(w)\n",
    "    w_sum = np.sum(w_abs)\n",
    "    ent_int = []\n",
    "    \n",
    "    for i in w_abs:\n",
    "        if i ==0:\n",
    "            ent_i = 0\n",
    "        else:\n",
    "            ent_i = -(i/w_sum)*(log(i/w_sum))\n",
    "        ent_int.append(ent_i)\n",
    "    entropy_initial = np.sum(ent_int)\n",
    "    entropy_initial = round(entropy_initial,4)\n",
    "    return entropy_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da3c5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADHD_data = ADHD_data\n",
    "Control_data = TD_data\n",
    "\n",
    "ring_topology_lists = pd.read_csv(\"{}/ring_topology_5_clusters.csv\".format(path))\n",
    "ring_nodes = pd.read_csv(\"{}/ring_nodes_5_clusters.csv\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ce1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(nodes, edge_strings, edge_values):\n",
    "    # Initialize a matrix with zeros\n",
    "    adj_matrix = np.zeros((len(nodes), len(nodes)))\n",
    "\n",
    "    # Set diagonal elements to 1\n",
    "    np.fill_diagonal(adj_matrix, 1)\n",
    "\n",
    "    # Check if the length of edge_strings and edge_values are the same\n",
    "    if len(edge_strings) != len(edge_values):\n",
    "        raise ValueError(\"The length of edge_strings and edge_values must be the same\")\n",
    "\n",
    "    # Parse and add edges with values\n",
    "    for edge_str, value in zip(edge_strings, edge_values):\n",
    "        # Extract nodes from the edge string\n",
    "        node1, node2 = map(int, edge_str.strip('()').split(', '))\n",
    "        # Adjust indices to match matrix indexing (starting from 0)\n",
    "        index1, index2 = nodes.index(node1), nodes.index(node2)\n",
    "        # Set the corresponding positions in the matrix to the given edge value\n",
    "        adj_matrix[index1][index2] = adj_matrix[index2][index1] = value\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7735bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADHD_ee_all = np.empty((len(ADHD_data),0))\n",
    "Control_ee_all = np.empty((len(Control_data),0))\n",
    "\n",
    "ring_index = 0\n",
    "\n",
    "for i in range(len(ring_topology_lists)):\n",
    "    ring_index += 1\n",
    "    topology_list = list(ring_topology_lists.iloc[i])\n",
    "    topology_list_clear = [item for item in topology_list if not pd.isna(item) ]\n",
    "\n",
    "    node_list = list(ring_nodes.iloc[i])\n",
    "    node_list_clear = [int(item) for item in node_list if not pd.isna(item) ]\n",
    "    \n",
    "    \n",
    "    ADHD_ee_each_ring = []\n",
    "    \n",
    "    ADHD_data_each_topology = np.array(ADHD_data[topology_list_clear])\n",
    "    for j in range(len(ADHD_data_each_topology)):\n",
    "        ADHD_data_sample_values = list(ADHD_data_each_topology[j])\n",
    "        ADHD_sample_matrix = create_adjacency_matrix(node_list_clear,topology_list_clear, ADHD_data_sample_values)\n",
    "        ADHD_ee_sample = eigen_entropy(ADHD_sample_matrix)\n",
    "        ADHD_ee_each_ring.append(ADHD_ee_sample)\n",
    "    \n",
    "    ADHD_ee_each_ring_np = np.array(ADHD_ee_each_ring).reshape(-1,1)\n",
    "    \n",
    "    ADHD_ee_all = np.concatenate((ADHD_ee_all,ADHD_ee_each_ring_np),axis = 1)\n",
    "        \n",
    "    Control_ee_each_ring = []\n",
    "    \n",
    "    Control_data_each_topology = np.array(Control_data[topology_list_clear])\n",
    "    for j in range(len(Control_data_each_topology)):\n",
    "        Control_data_sample_values = list(Control_data_each_topology[j])\n",
    "        Control_sample_matrix = create_adjacency_matrix(node_list_clear,topology_list_clear, Control_data_sample_values)\n",
    "        Control_ee_sample = eigen_entropy(Control_sample_matrix)\n",
    "        Control_ee_each_ring.append(Control_ee_sample)\n",
    "    \n",
    "    Control_ee_each_ring_np = np.array(Control_ee_each_ring).reshape(-1,1)\n",
    "    Control_ee_all = np.concatenate((Control_ee_all,Control_ee_each_ring_np),axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a4a01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EE_features_names = []\n",
    "\n",
    "for i in range(1, len(ADHD_ee_all[0])+1):\n",
    "    EE_features_names.append(\"EE(S{})\".format(i))\n",
    "\n",
    "\n",
    "ADHD_ee_all_fr = pd.DataFrame(ADHD_ee_all, columns = EE_features_names)\n",
    "Control_ee_all_fr = pd.DataFrame(Control_ee_all, columns = EE_features_names)\n",
    "\n",
    "ADHD_ee_all_fr.to_csv(\"{}/ADHD_node_nets_5_clusters.csv\".format(path), index=False)\n",
    "Control_ee_all_fr.to_csv(\"{}/Control_node_nets_5_clusters.csv\".format(path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418343f",
   "metadata": {},
   "source": [
    "# Step 5. Assessment by Machine Learning (KNN classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd29efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa7a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADHD_data = pd.read_csv(\"{}/ADHD_node_nets_5_clusters.csv\".format(path))\n",
    "ADHD_label = []\n",
    "\n",
    "for i in range(len(ADHD_data)):\n",
    "    ADHD_label.append(1)\n",
    "\n",
    "    \n",
    "Control_data = pd.read_csv(\"{}/Control_node_nets_5_clusters.csv\".format(path))\n",
    "Control_label = []\n",
    "\n",
    "for i in range(len(Control_data)):\n",
    "    Control_label.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb243254",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADHD_data = np.array(ADHD_data)\n",
    "ADHD_label = np.array(ADHD_label).reshape(-1,1)\n",
    "\n",
    "\n",
    "Control_data = np.array(Control_data)\n",
    "Control_label = np.array(Control_label).reshape(-1,1)\n",
    "\n",
    "\n",
    "data = np.concatenate((ADHD_data, Control_data), axis = 0)\n",
    "labels = np.concatenate((ADHD_label, Control_label), axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ab15908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: 21\n"
     ]
    }
   ],
   "source": [
    "neighbors = []\n",
    "\n",
    "for i in range(3, 31):\n",
    "    neighbors.append(i)\n",
    "\n",
    "X_train_std = data\n",
    "y_train = labels\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "clf = GridSearchCV(knn, cv = 5, param_grid = {'n_neighbors': neighbors})\n",
    "clf.fit(data,labels.ravel())\n",
    "print(\"Best parameters:\",clf.best_params_['n_neighbors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c607bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "X_train = data\n",
    "y_train = labels\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = clf.best_params_['n_neighbors'])\n",
    "\n",
    "kf = KFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "all_accuracy = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    data_cv_train, data_cv_val = X_train[train_index], X_train[val_index]\n",
    "    label_cv_train, label_cv_val = y_train[train_index], y_train[val_index]\n",
    "    knn.fit(data_cv_train, label_cv_train.ravel())\n",
    "    y_train_pred = knn.predict(data_cv_train)\n",
    "    acc_train = accuracy_score(label_cv_train, y_train_pred)\n",
    "    y_val_pred = knn.predict(data_cv_val)\n",
    "    acc_val = accuracy_score(label_cv_val, y_val_pred)\n",
    "    all_accuracy.append(acc_val)\n",
    "\n",
    "\n",
    "print(\"Average Accuracy:\",round(np.mean(all_accuracy),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d86b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c82a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
